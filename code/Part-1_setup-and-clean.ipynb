{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7a73bfa-6f77-4c06-a275-9ab8c592df19",
   "metadata": {},
   "source": [
    "# Project 3: Subreddit Classification with Pushshift API and NLP\n",
    "\n",
    "## Part I - Project Intro and Data Cleaning\n",
    "\n",
    "Author: Charles Ramey\n",
    "\n",
    "Date: 04/02/2023\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3493678f-85b0-4693-853a-a9aed79cacda",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "\n",
    "Hockey For Everyone (\"HFE\") is a fictional startup company whose mission is to design and sell affordable, high-quality hockey equipment that meets the needs of players of all backgrounds and abilities. This project aims to determine the most effective way to market hockey gear to reddit users by analyzing two subreddits: r/hockey and r/hockeyplayers. The goal is to identify which subreddit tends to focus more on non-professional hockey by analyzynig common words and phrases used in submissions to each subreddit. The project explores data scraped from Reddit using the Pushshift API, and tests a variety of classification machine learning models to accurately submissions by subreddit and provide the HFE marketing team with a tool to test advertisement language before rollout. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e02f55-362c-4b7b-a6e0-46572771ab94",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Notebook Links\n",
    "\n",
    "Part II - Exploratory Data Analysis (EDA)\n",
    "- [`Part-2_eda.ipynb`](../code/Part-1_eda.ipynb)\n",
    "\n",
    "Part III - Modeling\n",
    "- [`Part-3_modeling.ipynb`](../code/Part-3_modeling.ipynb)\n",
    "\n",
    "Part IV - Conclusion, Recommendations, and Sources\n",
    "- [`Part-4_conclusion-and-recommendations.ipynb`](../code/Part-4_conclusion-and-recommendations.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1db3b3-860b-4947-ad16-04097ac5d4eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Contents\n",
    "\n",
    "- [Background](#Background)\n",
    "- [Data Import](#Data-Import)\n",
    "- [Cleaning](#Cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b7a556-b48e-40dc-9a2e-2d8a1ea6dd42",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Reddit is a popular social media platform with a large and diverse user base. Companies often use Reddit to market their products to specific communities, or subreddits. For HFE, idenfitying the most relevant subreddit to target is critical to the success of their marketing campaign. By analyzing the language used in submissions to [r/hockey](https://www.reddit.com/r/hockey/) and [r/hockeyplayers](https://www.reddit.com/r/hockeyplayers/), the company can gain insight into the interests and needs of its potential customers. The project will use [natural language processing (NLP)](https://www.ibm.com/topics/natural-language-processing) to identify distinguishing patterns and trends in the language used in the two hockey-centric subreddits.\n",
    "\n",
    "Data from the two subreddits is collected using the [Pushshift API](https://medium.com/mcd-unison/using-pushshift-api-for-data-analysis-on-reddit-b08d339c48b8) and the Python [`requests`] library. Pushshift is a platform that collects and archives a variety of Reddit data, from the number of posts in a subreddit to the amount of upvotes a comment has received. Users of the API can obtain the public data they are searching for by using the necessary search endpoint url in conjunction with a number of different [query parameters](http://api.pushshift.io/redoc#operation/search_reddit_subreddits_reddit_search_subreddit_get). In this project, approximately 200,000 of the most recent submissions were scraped from the two subreddits up until March 28, 12:00PM. The data was then cleaned and condensed to about 62,000 which were then used to train and test a variety of classification machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e03f0f-822d-4852-96fa-6eae2f1d8395",
   "metadata": {},
   "source": [
    "### Disclaimer\n",
    "\n",
    "During the creation of this notebook and the analysis contained herein, the author frequently referenced available documentation for imported libraries. Much of the coding syntax within this notebook is derived from various online sources, including stackoverflow, other public forums, and OpenAI's ChatGPT. The author does not claim the following code as fully original, and sources are cited where possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc3a5b2-62c9-4972-a30f-198c8f48ad2c",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "964855c1-528c-4270-8479-fa022f13211d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9995dd3b-f56d-4c9b-acbd-84bcc09f344d",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1672815-a5fe-46bc-ba4f-43f2375da121",
   "metadata": {},
   "source": [
    "This section will use the Pushshift API to collect the initial uncleaned data for both subreddits. The analysis will focus only on submissions (posts) and will not consider comments. Thus, the Pushshift endpoint for submission data is used as the base URL to which additional query parameters will be appended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfaf7a78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://api.pushshift.io/reddit/search/submission'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# establish base url for submission endpoint\n",
    "base_url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "base_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4982b3-f100-4812-9c34-e136525c4a0f",
   "metadata": {},
   "source": [
    "For submission time stamps, the Pushshift API works in [Unix epochs](https://www.epochconverter.com/), a proxy for coordinated universal time (UTC). The code cells below establish a couple things:\n",
    "\n",
    "1. `current_time` returns the Unix epoch timestamp the corresponds to the time at which this notebook is last run.\n",
    "2. `before_time` defines the starting point for the window over which this project will analyze submission. As previously stated, this Unix epoch timestamp corresponds to March 28, 2023 at 12:00pm.\n",
    "3. The following cell prints out the number of days after March 28, 2023 at 12:00pm that this notebook is being run. This is to give the viewer a relative idea of the time frame of consideration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "535aaca6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1680456746"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the current time (as unix epoch timestamp)\n",
    "current_time = round(time.time())\n",
    "current_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a31d185-1aaa-4d0b-99cb-f8114df444b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1680019200"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set start time for submission collection\n",
    "# For this project, we will look at the submissions leading up to\n",
    "# March 28, 2023 at 12:00pm, noon.\n",
    "# The Unix epoch for this datetime is 1680019200\n",
    "before_time = 1680019200\n",
    "before_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f8e1372-dc95-4dc1-a176-bf0fbb26e8e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This notebook was run 5.06 days after the studied time frame.\n"
     ]
    }
   ],
   "source": [
    "print(f\"This notebook was run {round((current_time-before_time)/(24*60*60), 2)} days after the studied time frame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67d26f50-118a-4cb9-8e46-421776c77723",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This cell will limit dataframe displays to 10 columns for readability\n",
    "pd.set_option('display.max_columns', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a60c7c-050a-41bb-be66-3fdc67add0fd",
   "metadata": {},
   "source": [
    "#### r/hockey\n",
    "\n",
    "The following code cells implement the Pushshift API via looping to collected the necessary submission data for the r/hockey subreddit. The data is saved to a Pandas dataframe under the alias, `h`, for r/`h`ockey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18633ff5-449d-4250-870d-2486b474bd7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 149940 total submissions with a scrape time of 3.3522249921824403 seconds per 1000 submissions.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Code adapted from Devin Faye, General Assembly\n",
    "'''\n",
    "# initializing dataframe for r/hockey\n",
    "h = []\n",
    "\n",
    "# initializing variable to store the total time required to scrape the data\n",
    "total_time = 0\n",
    "\n",
    "# setting query parameters for Pushshift API request\n",
    "params_h = {\n",
    "    'subreddit' : 'hockey', # select submissions from r/hockey\n",
    "    'limit' : 1000, # limit selection to 1000 submissions\n",
    "    'until' : before_time, # start scraping at 03-28-2023 12:00:00 and work backwards in time\n",
    "    'sort' : 'created_utc', # sort the data time-wise\n",
    "    'order' : 'desc' # order in descending order (most recent to oldest)\n",
    "}\n",
    "\n",
    "# Creating a for loop to iterate over the 1000 submission limit until\n",
    "# the desired number of submissions have been collected.\n",
    "# The range is set to 150 to collect 150,000 submissions.\n",
    "# This number was selected through iteration, such that the cleaned dataframes\n",
    "# for both subreddits will contain approximately equal submission counts.\n",
    "for _ in range(150):\n",
    "    # setting a loop start time to calculate time per loop\n",
    "    start = time.time()\n",
    "    # Rather than print each response code to make sure the requests are working\n",
    "    # We will raise an exception if the response code is not 200\n",
    "    try:\n",
    "        # sending request to Pushshift URL with chosen query parameters\n",
    "        res_h = requests.get(base_url, params=params_h)\n",
    "        if str(res_h) != '<Response [200]>':\n",
    "            raise Exception(\"Response code is not 200\")\n",
    "    except Exception as e:\n",
    "        print(\"An exception occured:\", e)\n",
    "    # Saving the 1000 requested submissions to a dataframe\n",
    "    posts = pd.DataFrame(res_h.json()['data'])\n",
    "    # Adding the pulled data to the master dataframe, h\n",
    "    h += res_h.json()['data']\n",
    "    # Updating the new scraping window start time to be the end of the previous iteration\n",
    "    params_h['until'] = posts['created_utc'].min()\n",
    "    # Setting a loop end time to calculate time per loop\n",
    "    end = time.time()\n",
    "    # Caluclating time to scrape the max number of submissions (1000) for this iteration\n",
    "    time_to = end - start\n",
    "    # Adding the scrape time of this loop to the total\n",
    "    total_time += time_to\n",
    "    # Wait for one second before making the next request to avoid overloading the API\n",
    "    time.sleep(1)\n",
    "        \n",
    "# Print the number of submissions retrieved and the rate of retrieval\n",
    "print(f\"Retrieved {len(h)} total submissions with a scrape time of {total_time / (len(h) / 1000)} seconds per 1000 submissions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb60b993-176e-4f6f-85bf-83d5c940e1f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert the result to a pandas dataframe\n",
    "h = pd.DataFrame(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8eca0ce5-42bd-406a-bbe4-489f3fb67590",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((149940, 106), 149940)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify that there are no duplicates\n",
    "h.shape, h.id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2ee0c1-18ed-4140-8bf6-55f9e87c5daa",
   "metadata": {},
   "source": [
    "#### r/hockeyplayers\n",
    "\n",
    "The following code cells implement the Pushshift API via looping to collected the necessary submission data for the r/hockeyplayers subreddit. The data is saved to a Pandas dataframe under the alias, `hp`, for r/`h`ockey`p`layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f74c3194-39f7-487e-9869-d9ec2881d273",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 49987 total submissions with a scrape time of 2.368637295739617 seconds per 1000 submissions.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Code adapted from Devin Faye, General Assembly\n",
    "'''\n",
    "# initializing dataframe for r/hockeyplayers\n",
    "hp = []\n",
    "\n",
    "# initializing variable to store the total time required to scrape the data\n",
    "total_time = 0\n",
    "\n",
    "# setting query parameters for Pushshift API request\n",
    "params_hp = {\n",
    "    'subreddit' : 'hockeyplayers', # select submission from r/hockeyplayers\n",
    "    'limit' : 1000, # limit selection to 1000 submissions\n",
    "    'until' : before_time, # start scraping at 03-28-2023 12:00:00 and work backwards in time\n",
    "    'sort' : 'created_utc', # sort the data time-wise\n",
    "    'order' : 'desc' # order in descending order (most recent to oldest)\n",
    "}\n",
    "\n",
    "# Creating a for loop to iterate over the 1000 submission limit until\n",
    "# the desired number of submissions have been collected.\n",
    "# The range is set to 50 to collect 50,000 submissions.\n",
    "# This number was selected because the r/hockeyplayers subreddit\n",
    "# has just over 50,000 total submissions since it was created..\n",
    "for _ in range(50):\n",
    "    # setting a loop start time to calculate time per loop\n",
    "    start = time.time()\n",
    "    # Rather than print each response code to make sure the requests are working\n",
    "    # We will raise an exception if the response code is not 200\n",
    "    try:\n",
    "        # sending request to Pushshift URL with chosen query parameters\n",
    "        res_hp = requests.get(base_url, params=params_hp)\n",
    "        if str(res_hp) != '<Response [200]>':\n",
    "            raise Exception(\"Response code is not 200\")\n",
    "    except Exception as e:\n",
    "        print(\"An exception occured:\", e)\n",
    "    # Saving the 1000 requested submissions to a dataframe\n",
    "    posts = pd.DataFrame(res_hp.json()['data'])\n",
    "    # Adding the pulled data to the master dataframe, h\n",
    "    hp += res_hp.json()['data']\n",
    "    # Updating the new scraping window start time to be the end of the previous iteration\n",
    "    params_hp['until'] = posts['created_utc'].min()\n",
    "    # Setting a loop end time to calculate time per loop\n",
    "    end = time.time()\n",
    "    # Caluclating time to scrape the max number of submissions (1000) for this iteration\n",
    "    time_to = end - start\n",
    "    # Adding the scrape time of this loop to the total\n",
    "    total_time += time_to\n",
    "    # Wait for a second before making the next request to avoid overloading the API\n",
    "    time.sleep(1)\n",
    "        \n",
    "# Print the number of submissions retrieved and the rate of retrieval\n",
    "print(f\"Retrieved {len(hp)} total submissions with a scrape time of {total_time / (len(hp) / 1000)} seconds per 1000 submissions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "340b633a-6961-439e-a59f-cc15cb364cdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# convert the result to a pandas dataframe\n",
    "hp = pd.DataFrame(hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "525bb91b-ca2d-4af7-b244-3d1f8d1b3184",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((49987, 122), 49987)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# verify that there are no duplicates\n",
    "hp.shape, hp.id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6a1b9e-337c-4846-9828-15350ccbdaac",
   "metadata": {},
   "source": [
    "---\n",
    "## Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fc42e6-e488-4aaf-b4db-93f51423c50a",
   "metadata": {},
   "source": [
    "In this section, the initial dataframes are cleaned and condensed, removing unwanted and unnecessary data. The r/hockey and r/hockeyplayers dataframes are ultimately reduced to two columns, `subreddit` and `text`, and combined into a single, common dataframe, `df`, which will be used for data exploration and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf9c813a-0c20-4c61-ba71-1825ee91d154",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>gilded</th>\n",
       "      <th>title</th>\n",
       "      <th>...</th>\n",
       "      <th>tournament_data</th>\n",
       "      <th>event_end</th>\n",
       "      <th>event_is_live</th>\n",
       "      <th>event_start</th>\n",
       "      <th>removal_reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hockey</td>\n",
       "      <td>I haven't been able to find out whether he is ...</td>\n",
       "      <td>t2_dxf3o</td>\n",
       "      <td>0</td>\n",
       "      <td>Does anyone know Jordan Staals take on Pride j...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hockey</td>\n",
       "      <td></td>\n",
       "      <td>t2_v2ee6yxk</td>\n",
       "      <td>0</td>\n",
       "      <td>Controversial ref call in the 6:th quarter fin...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit                                           selftext  \\\n",
       "0    hockey  I haven't been able to find out whether he is ...   \n",
       "1    hockey                                                      \n",
       "\n",
       "  author_fullname  gilded                                              title  \\\n",
       "0        t2_dxf3o       0  Does anyone know Jordan Staals take on Pride j...   \n",
       "1     t2_v2ee6yxk       0  Controversial ref call in the 6:th quarter fin...   \n",
       "\n",
       "   ... tournament_data event_end  event_is_live  event_start removal_reason  \n",
       "0  ...             NaN       NaN            NaN          NaN            NaN  \n",
       "1  ...             NaN       NaN            NaN          NaN            NaN  \n",
       "\n",
       "[2 rows x 106 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bec45d4d-d467-4426-b88b-dc09d5563d9e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>selftext</th>\n",
       "      <th>author_fullname</th>\n",
       "      <th>gilded</th>\n",
       "      <th>title</th>\n",
       "      <th>...</th>\n",
       "      <th>approved_at_utc</th>\n",
       "      <th>banned_at_utc</th>\n",
       "      <th>from_kind</th>\n",
       "      <th>from_id</th>\n",
       "      <th>from</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hockeyplayers</td>\n",
       "      <td>Has anyone tried the new Warrior Ritual X4 Pro...</td>\n",
       "      <td>t2_vipi2zpy</td>\n",
       "      <td>0</td>\n",
       "      <td>Warrior Goalie Pants</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hockeyplayers</td>\n",
       "      <td></td>\n",
       "      <td>t2_d1md0ej</td>\n",
       "      <td>0</td>\n",
       "      <td>Goalies, any tips for a player going in net fo...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       subreddit                                           selftext  \\\n",
       "0  hockeyplayers  Has anyone tried the new Warrior Ritual X4 Pro...   \n",
       "1  hockeyplayers                                                      \n",
       "\n",
       "  author_fullname  gilded                                              title  \\\n",
       "0     t2_vipi2zpy       0                               Warrior Goalie Pants   \n",
       "1      t2_d1md0ej       0  Goalies, any tips for a player going in net fo...   \n",
       "\n",
       "   ... approved_at_utc banned_at_utc from_kind  from_id from  \n",
       "0  ...             NaN           NaN       NaN      NaN  NaN  \n",
       "1  ...             NaN           NaN       NaN      NaN  NaN  \n",
       "\n",
       "[2 rows x 122 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a8a4520-8132-42eb-915b-a13bdecb9ed8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hockey                 149905\n",
       "u_valhalla-hockey          27\n",
       "u_rl-hockey-god             4\n",
       "u_Howitzer-Hockey           1\n",
       "u_All-things-hockey         1\n",
       "u_gratts-hockey             1\n",
       "u_All-Hockey                1\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking that hockey is the only class represented in the subreddit column\n",
    "h['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a858c7fc-2977-4d4e-a57a-98e8d0d0b4b3",
   "metadata": {},
   "source": [
    "Whether due to an issue with the API or some fault with the request code, there are some submissions that are incorrectly labeled as not belonging to the subreddit r/hockey. Since these submissions make up only a small fraction of the data that was pulled, we will simply drop these submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fb0a7f7-2751-4f28-92ec-fa20e9fc3fdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Using only submissions with the correct subreddit nam\n",
    "h = h[h['subreddit'] == 'hockey']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b61ba30-134a-41d4-a581-9fe1610f2904",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hockeyplayers    49987\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking that hockeyplayers is the only class represented in the subreddit column\n",
    "hp['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "689ca351-1f2f-4305-9e53-56de5e61dea0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "removal_reason                   149905\n",
       "discussion_type                  149905\n",
       "view_count                       149905\n",
       "removed_by                       149905\n",
       "content_categories               149905\n",
       "event_start                      149904\n",
       "event_is_live                    149904\n",
       "event_end                        149904\n",
       "tournament_data                  149904\n",
       "top_awarded_type                 149880\n",
       "distinguished                    149822\n",
       "poll_data                        149796\n",
       "call_to_action                   149735\n",
       "category                         149735\n",
       "author_cakeday                   149396\n",
       "gallery_data                     149130\n",
       "media_metadata                   148711\n",
       "is_gallery                       148419\n",
       "crosspost_parent_list            145943\n",
       "crosspost_parent                 145943\n",
       "suggested_sort                   144354\n",
       "edited_on                        141575\n",
       "link_flair_template_id           121884\n",
       "link_flair_css_class             102029\n",
       "link_flair_text                  101455\n",
       "author_flair_template_id          99459\n",
       "media                             98146\n",
       "secure_media                      98146\n",
       "post_hint                         89670\n",
       "preview                           89670\n",
       "removed_by_category               87956\n",
       "retrieved_on                      79436\n",
       "author_flair_css_class            66699\n",
       "author_flair_text                 66694\n",
       "url_overridden_by_dest            65092\n",
       "thumbnail_width                   64222\n",
       "thumbnail_height                  64222\n",
       "awarders                          59471\n",
       "subreddit_name_prefixed           59471\n",
       "author_created_utc                51752\n",
       "author_flair_background_color     35310\n",
       "author_flair_text_color           35287\n",
       "author_patreon_flair              31407\n",
       "author_fullname                   31407\n",
       "author_flair_type                 31407\n",
       "author_flair_richtext             31407\n",
       "author_premium                    31407\n",
       "is_created_from_ads_ui            10537\n",
       "hide_score                        10537\n",
       "upvote_ratio                      10400\n",
       "link_flair_text_color              1568\n",
       "link_flair_background_color        1520\n",
       "url                                 444\n",
       "domain                              444\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the features with one or more null values and the number of null values they contain\n",
    "h.isna().sum().sort_values(ascending = False).loc[lambda x: x > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61d0b55e-8bb4-48fc-88ef-60c2433e1788",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# From a quick review, none of these features will be relevant to this analyis\n",
    "# So they will be dropped from the dataframe before further cleaning\n",
    "h = h.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5910bd7f-02d4-4dbc-8179-f424dea200a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "from               49987\n",
       "likes              49987\n",
       "link_flair_text    49987\n",
       "view_count         49987\n",
       "removed_by         49987\n",
       "                   ...  \n",
       "spoiler             4751\n",
       "contest_mode        4387\n",
       "domain                90\n",
       "url                   90\n",
       "archived              71\n",
       "Length: 100, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List the features with one or more null values and the number of null values they contain\n",
    "hp.isna().sum().sort_values(ascending = False).loc[lambda x: x > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0bd81044-f4ff-48a4-82c8-2b72eaa0ecaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Dropping columns with null values\n",
    "hp = hp.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2aa1c161-bee6-499d-ad72-4ba6a29ba639",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Checking for common columns between the two dataframes\n",
    "common_cols = h.columns.intersection(hp.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22f0f75a-cf33-4ea5-9b59-4d1ed82b6d16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# From here on, we will only clean based on shared features\n",
    "h = h.reindex(columns=common_cols)\n",
    "hp = hp.reindex(columns=common_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0c4dc5c-bb04-4208-a6ce-b0f257df600c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((149905, 22), (49987, 22))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape, hp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "925b51ba-5029-43af-b820-5ccc5d2c0465",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['subreddit', 'selftext', 'gilded', 'title', 'media_embed',\n",
       "       'secure_media_embed', 'score', 'thumbnail', 'edited', 'is_self',\n",
       "       'over_18', 'locked', 'subreddit_id', 'id', 'author', 'num_comments',\n",
       "       'permalink', 'stickied', 'created_utc', 'retrieved_utc', 'updated_utc',\n",
       "       'utc_datetime_str'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21c909d6-7fb8-4e7f-8685-72b40806006d",
   "metadata": {},
   "source": [
    "The `is_self` field represents submissions that are self posts (i.e. not reposted/cross-posted). We want to use these posts to help minimize duplicates and focus on users who are posting original content relevent to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "663fa254-e196-49b2-bb5b-6559dab6b2ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    91264\n",
       "True     58641\n",
       "Name: is_self, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see how many posts were created by the submitting user\n",
    "h['is_self'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21e16e9d-9cef-4cc0-9c5a-491e32a2a547",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     35354\n",
       "False    14633\n",
       "Name: is_self, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to see how many posts were created by the submitting user\n",
    "hp['is_self'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af8ff40f-f46e-4363-9a65-2c1c93341d8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Let's drop the non-self submissions\n",
    "h = h.drop(h[h['is_self'] == False].index)\n",
    "hp = hp.drop(hp[hp['is_self'] == False].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9eab76ad-c218-4395-a4bd-8c77ed84d1bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now we'll reduce the dataframe to just the subreddit, title and, body text\n",
    "# These are the fields that will be used for training our model in Part 2\n",
    "h = h[['subreddit', 'title', 'selftext']]\n",
    "hp = hp[['subreddit', 'title', 'selftext']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "28110c73-2213-453d-93c1-31701eeceaab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# To get the most text to train the model, \n",
    "# we will combine the title and selftext fields into a single text column\n",
    "h['text'] = h['title'] + ' ' + h['selftext']\n",
    "hp['text'] = hp['title'] + ' ' + hp['selftext']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "00ddf880-8129-43f9-ae6a-d80ba4c29d59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Now that we've engineered a new feature from title and selftext,\n",
    "# We will remove these fields from the dataframe\n",
    "h = h.drop(columns=['title','selftext'])\n",
    "hp = hp.drop(columns=['title','selftext'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "779b8451-2dd4-47c4-a362-1ba36e77e12c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hockey</td>\n",
       "      <td>Does anyone know Jordan Staals take on Pride j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hockey</td>\n",
       "      <td>[Game Thread][Hockey Federation of Ukraine Cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hockey</td>\n",
       "      <td>Elimination/Clinching Scenarios + Daily Free T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hockey</td>\n",
       "      <td>Is McDavid's Contract Worth It? I'm guessing h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hockey</td>\n",
       "      <td>How’s the Tank for Bedard? Who’s the favourite...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  subreddit                                               text\n",
       "0    hockey  Does anyone know Jordan Staals take on Pride j...\n",
       "2    hockey  [Game Thread][Hockey Federation of Ukraine Cha...\n",
       "3    hockey  Elimination/Clinching Scenarios + Daily Free T...\n",
       "4    hockey  Is McDavid's Contract Worth It? I'm guessing h...\n",
       "9    hockey  How’s the Tank for Bedard? Who’s the favourite..."
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e6fa17d2-3c59-4a42-860e-f18a0c3f1ded",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hockeyplayers</td>\n",
       "      <td>Warrior Goalie Pants Has anyone tried the new ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hockeyplayers</td>\n",
       "      <td>Goalies, any tips for a player going in net fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hockeyplayers</td>\n",
       "      <td>Huge Hockey Facility up for Auction Lots and l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hockeyplayers</td>\n",
       "      <td>Skates for kids with wide feet Hope I'm in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hockeyplayers</td>\n",
       "      <td>Gear for sled hockey Posting here for a friend...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       subreddit                                               text\n",
       "0  hockeyplayers  Warrior Goalie Pants Has anyone tried the new ...\n",
       "1  hockeyplayers  Goalies, any tips for a player going in net fo...\n",
       "2  hockeyplayers  Huge Hockey Facility up for Auction Lots and l...\n",
       "4  hockeyplayers  Skates for kids with wide feet Hope I'm in the...\n",
       "6  hockeyplayers  Gear for sled hockey Posting here for a friend..."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0293b0f7-0cda-45a5-81d2-b0f7483810de",
   "metadata": {},
   "source": [
    "The most useful submissions are those which contain more text and context. We may still get some useful phrases from two-word posts(for example, \"new skates\"), however one-word submissions are not expected to be especially useful, therefor they will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f58d7c2-9c33-4444-9a49-a6570d09095f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get a word count columns\n",
    "h['word_count'] = h['text'].apply(lambda x: len(x.split()))\n",
    "hp['word_count'] = hp['text'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3071abee-f7fb-4bb6-a091-6f712870d258",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop submissions with 1 word\n",
    "h = h[h['word_count'] > 1]\n",
    "hp = hp[hp['word_count'] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e5b15d5f-8abc-4ec5-95c9-971d04307905",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop word count column since it is no longer needed\n",
    "h = h.drop(columns='word_count')\n",
    "hp = hp.drop(columns='word_count')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391c8ec6-ae52-4212-b728-d39baac59709",
   "metadata": {},
   "source": [
    "With the Pushshift API, it is possible to get submissions which have deleted or removed content. Not only are these submissions unhelpful, but they are common, so the words \"deleted\" and \"removed\" are likely to show up frequently in both subreddits. We will remove these submissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "754da4a6-a6d6-4121-b7ca-80423a486e65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# drop submissions that have been deleted/removed\n",
    "h = h[~h['text'].str.contains('\\[deleted]|\\[removed]')]\n",
    "hp = hp[~hp['text'].str.contains('\\[deleted]|\\[removed]')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5761996c-c57c-4ad6-8a5f-6d0376d6c132",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32177, 2), (30726, 2))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape, hp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534a2812-dc68-40a4-a8b1-2026dc07adf1",
   "metadata": {},
   "source": [
    "Now the dataframes have been sufficiently cleaned and both subreddits are approximately equally represented, with just over 30,000 submissions each. Up to this point, we've been removing entire rows from the data, however all further cleaning will be performed on the text itself to prepare it for EDA and modeling. We will combined the dataframes at this point and save the combined dataframe before manipulating the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a6992bd5-e6f4-4fce-a2a3-995c89040153",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Combine the dataframes into a single dataframe for preprocessing\n",
    "df = pd.concat([h, hp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "25562896-4dcc-4800-85e6-db43ee90425c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hockey           32177\n",
       "hockeyplayers    30726\n",
       "Name: subreddit, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['subreddit'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08771d10-c007-4904-932a-ae8ddf8d20db",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Save Cleaned Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c261b517-f769-4f49-9ceb-d4393372e39c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.to_csv('../data/combined_text.csv', index=False)\n",
    "h.to_csv('../data/hockey_text.csv', index=False)\n",
    "hp.to_csv('../data/hockeyplayers_text.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf6c04b-5f3e-43fe-ac2b-3d4a0d8005b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "#### Notebook Links\n",
    "\n",
    "Part II - Exploratory Data Analysis (EDA)\n",
    "- [`Part-2_eda.ipynb`](../code/Part-1_eda.ipynb)\n",
    "\n",
    "Part III - Modeling\n",
    "- [`Part-3_modeling.ipynb`](../code/Part-3_modeling.ipynb)\n",
    "\n",
    "Part IV - Conclusion, Recommendations, and Sources\n",
    "- [`Part-4_conclusion-and-recommendations.ipynb`](../code/Part-4_conclusion-and-recommendations.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
